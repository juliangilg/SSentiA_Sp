{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo SSentiA_Sp model for CorpusCine dataset\n",
    "\n",
    "This problem is to design an automated model for sentiment analysis in Spanish with no labels. Specifically we test our apporach in CorpusCine dataset, which is a dataset formed by 3878 Spanish-written movie reviews captured from the MuchoCine website (\\url{https://muchocine.net/}). Each document is rated using an integer tag ranging from 1 (unpleasant movie) to 5 (excellent movie). We use a methodology to treat PaperReviews as a binary classification problem. Samples with rating one or two are considered as negatives reviews; similarly, documents with rating four or five are categorized as positive reviews. This dataset is publicy avilable in http://www.lsi.us.es/~fermin/corpusCine.zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip setuptools wheel\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install deep-translator\n",
    "!python -m pip install urllib3[secure]\n",
    "!pip install spacytextblob\n",
    "!pip install vaderSentiment\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "sys.path.append('../Model')\n",
    "from SSentiA_Sp import sSentiA_Sp\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataName = 'MoviesReview'\n",
    "\n",
    "path = '../Data/' + DataName + '.xlsx'\n",
    "\n",
    "Data = pd.read_excel(path)\n",
    "numpy_array = Data.values\n",
    "X = numpy_array[:,0]\n",
    "Y = np.asarray(numpy_array[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Process the labels\n",
    "\n",
    "Originally, such dataset configures a 5-class classification problem, we convert it into a binary problem as follows: First, we discard samples with label equal to three. Moreover, samples with labels one and two are assigned to negative class (0); on the other hand, samples with labels four and fiv are categorized a positives (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[Y != 3]\n",
    "Y = Y[Y != 3]\n",
    "Y[Y < 3] = 0\n",
    "Y[Y > 3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Applying lexicon-based approaches to CorpusCine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Textblob lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRsentiA_TextBlob import LexicalAnalyzer\n",
    "r = LexicalAnalyzer('PaperReview')\n",
    "predictions, pred_confidence_scores = r.classify_binary_dataset(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 VADER lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRsentiA_VADER import LexicalAnalyzer\n",
    "r = LexicalAnalyzer('PaperReview')\n",
    "predictions, pred_confidence_scores = r.classify_binary_dataset(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spanish lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRSentiA_Spanish import LexicalAnalyzer\n",
    "r = LexicalAnalyzer('PaperReview')\n",
    "predictions, pred_confidence_scores = r.classify_binary_dataset(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Our hybrid approach\n",
    "\n",
    "Accordingly, in this work, we employ a self-supervised approach based on the Self-supervised Sentiment Analyzer for classification from unlabeled data--(SSentiA) . Such an approach generates pseudo-labels using a lexicon-based method; then, these labels are enhanced using a supervised classification scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 No labels\n",
    "\n",
    "We first test our approach under the scenario of having no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRsentiA_Sp import LexicalAnalyzer\n",
    "r = LexicalAnalyzer('PaperReview')\n",
    "predictions, pred_confidence_scores = r.classify_binary_dataset(X,Y)\n",
    "df1, df2, df3, df4, df5 = r.distribute_predictions_into_bins(X,Y,predictions, pred_confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sSentiA_Sp()\n",
    "s.apply_SSSentiA(df1, df2, df3, df4, df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Few labels\n",
    "\n",
    "Finally, aiming to evaluate the behavior of our hybrid proposal in scenarios with limited labeled data, we carry out an additional experiment, where we vary the number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervisedalgorithm import Logistic_Regression_Classifier, SVM_Classifier\n",
    "\n",
    "from supervisedalgorithm import  Performance\n",
    "from supervisedalgorithm  import TF_IDF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [df1, df2, df3, df4, df5]\n",
    "\n",
    "X  = np.array([1])\n",
    "Sc = np.array([1])\n",
    "Y  = np.array([1]) #true labels\n",
    "Z  = np.array([1])\n",
    "\n",
    "for i in range(5):\n",
    "    data = df[i]\n",
    "    content = data.values\n",
    "    X = np.concatenate((X, content[:,0]))\n",
    "    Sc = np.concatenate((Sc, content[:,3]))\n",
    "    Y = np.concatenate((Y, content[:,1]))\n",
    "    Z = np.concatenate((Z, content[:,2]))\n",
    "\n",
    "X, Sc, Y, Z = X[1:], Sc[1:], Y[1:], Z[1:]\n",
    "Y = Y.astype('int')\n",
    "Z = Z.astype('int')\n",
    "\n",
    "P = np.arange(0.05,0.9,0.1)\n",
    "N = len(P)\n",
    "Cla = ['LR', 'SVM']\n",
    "Data_1 = []\n",
    "for cl in Cla:\n",
    "    if cl == 'LR':\n",
    "        ml_classifier = Logistic_Regression_Classifier() \n",
    "    else:\n",
    "        ml_classifier = SVM_Classifier()\n",
    "    mean_F1 = np.zeros(N)\n",
    "    min_F1 = np.zeros(N)\n",
    "    max_F1 = np.zeros(N)\n",
    "    std_F1 = np.zeros(N)\n",
    "    for j, p in enumerate(P):\n",
    "        aux_F1 = np.zeros(5)\n",
    "        for i in range(5):\n",
    "            X_true, X_, _, Sc_, y_true, y_, _, Z_ = train_test_split(X, Sc, Y, Z, test_size=1-p, random_state=randint(100, 1000))\n",
    "            X1, Y1, Z1, X2, Y2, Z2, X3, Y3, Z3, X4, Y4, Z4, X5, Y5, Z5 = LexicalAnalyzer('PaperReview').distribute_predictions_into_bins_1(X_, y_, Z_, Sc_)\n",
    "            \n",
    "            X1 = np.concatenate((X_true, X1))\n",
    "            Y1 = np.concatenate((y_true, Y1))\n",
    "            Z1 = np.concatenate((y_true, Z1))\n",
    "            \n",
    "            bin_size_1_2 = len(X1) + len(X2) # + len(X_3) #+  len(X_01) + len(X_02) + len( X_11) + len(X_12)\n",
    "            print(\"---\",bin_size_1_2)\n",
    "            \n",
    "            \n",
    "            data = np.concatenate((X1,X2,X3), axis=None)\n",
    "            label = np.concatenate((Z1,Z2,Y3), axis=None)\n",
    "            \n",
    "            tf_idf = TF_IDF()\n",
    "            data = tf_idf.get_tf_idf(data)\n",
    "            \n",
    "            X_train = data[:bin_size_1_2]\n",
    "            Y_train = label[:bin_size_1_2]\n",
    "            \n",
    "            X_test = data[bin_size_1_2:]\n",
    "            Y_test = label[bin_size_1_2:]\n",
    "            \n",
    "            prediction_bin_3 = ml_classifier.predict(X_train, Y_train, X_test)\n",
    "    \n",
    "            print(\"Bin-3 Results\")\n",
    "            performance = Performance()\n",
    "            _,precision,  recall, f1_score, acc = performance.get_results(Y_test, prediction_bin_3)\n",
    "            print(\"Total: \", round(precision,4),  round(recall,4), round(f1_score,4),round(acc,4) )\n",
    "\n",
    "            data = np.concatenate((X1,X2,X3,X4,X5), axis=None)\n",
    "            label = np.concatenate((Z1,Z2,prediction_bin_3,Y4,Y5), axis=None)\n",
    "            \n",
    "            \n",
    "            tf_idf = TF_IDF()\n",
    "            data = tf_idf.get_tf_idf(data)\n",
    "        \n",
    "            bin_1_2_3_training_data = len(X1) + len(X2) + len(X3)  \n",
    "            \n",
    "            X_train = data[:bin_1_2_3_training_data]\n",
    "            Y_train = label[:bin_1_2_3_training_data]\n",
    "            \n",
    "            X_test = data[bin_1_2_3_training_data:]\n",
    "            Y_test = label[bin_1_2_3_training_data:]\n",
    "    \n",
    " \n",
    "            print(\"Bin-4results\")\n",
    "            prediction_bin_4_5 = ml_classifier.predict(X_train, Y_train, X_test)\n",
    "            _,precision,  recall, f1_score, acc = performance.get_results(Y_test[:len(X4)], prediction_bin_4_5[:len(X4)])\n",
    "            print(\"F1: \", round(precision,4),  round(recall,4), round(f1_score,4),round(acc,4) )\n",
    "            aux_F1[i] = acc\n",
    "            \n",
    "        mean_F1[j] = np.mean(aux_F1)\n",
    "        std_F1[j] = np.std(aux_F1)\n",
    "        min_F1[j] = mean_F1[j] - 2*np.std(aux_F1)\n",
    "        max_F1[j] = mean_F1[j] + 2*np.std(aux_F1)\n",
    "        \n",
    "    Data_ = np.concatenate((P.reshape(N,1), mean_F1.reshape(N,1), max_F1.reshape(N,1), min_F1.reshape(N,1)), axis=1)\n",
    "    Dat = pd.DataFrame(Data_,columns =None, index=None)\n",
    "    Data_1.append([P.reshape(N,1), mean_F1.reshape(N,1), std_F1.reshape(N,1)])\n",
    "#     Name_ = cl + '_Paper.dat'\n",
    "#     Dat.to_csv(Name_,index=False, header=False,sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(2):\n",
    "    plt.errorbar(Data_1[i][0].flatten(), Data_1[i][1].flatten(), yerr=Data_1[i][2].flatten(),capsize=4)\n",
    "plt.legend(['LR', 'SVM'])\n",
    "plt.title('PaperReview')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Ratio of labeled data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
