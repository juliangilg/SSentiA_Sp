{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo SSentiA_Sp model for CorpusCine dataset\n",
    "\n",
    "The third dataset comprises a collection of journalistic articles written by young university students in Colombia. This corpus includes news written by the 24 colleges media of the Colombian Network of College Journalism from 2001 to 2021. The dataset includes digital, printed, sound and audiovisual news, for a total of 3,049 news items and 3,451,486 words related to the armed conflict, the memory of the victims and the peace process in Colombia. For the case of this article, we will focus on the news-articles that appeared on the web pages from the 24 college media because they gave us greater diversity, greater reliability in the extraction process and greater impact due to its high production. However, we only used the news-articles from 8 college media, since they were the ones with more than 50 news items. In conclusion, a total of 2373 digital news-articles were analyzed. Finally, the documents' semantic orientations are generated by performing a lexicometry analyse using the Alceste-Reinert method for textual data clustering. Such dataset is publicy available (https://zenodo.org/record/6384840#.Yj5Av-fMJPZ). This confgures a more challenging setting due to we do not have access to the labels for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip setuptools wheel\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install deep-translator\n",
    "!python -m pip install urllib3[secure]\n",
    "!pip install spacytextblob\n",
    "!pip install vaderSentiment\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "sys.path.append('../Model')\n",
    "from SSentiA_caPAZ import sSentiA_Sp\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataName = 'universidadIbague_DigitalMedio'\n",
    "DataName = 'univalle_DigitalMedio'\n",
    "DataName = 'unisabana_DigitalMedio'\n",
    "DataName = 'uniminutoradio_DigitalMedio'\n",
    "DataName = 'tadeo_DigitalMedio'\n",
    "DataName = 'santiagoCali_DigitalMedio'\n",
    "DataName = 'rosario_DigitalMedio'\n",
    "DataName = 'autonomaBucaramanga_DigitalMedio'\n",
    "\n",
    "path = '../Data/caPAZ/' + DataName + '.xlsx'\n",
    "\n",
    "Data = pd.read_excel(path)\n",
    "numpy_array = Data.values\n",
    "X = np.asarray(numpy_array[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Applying lexicon-based approaches to ColombianConflict dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 No labels\n",
    "\n",
    "We first test our approach under the scenario of having no labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "$$$$\n",
      "0.41811317567400413 0.5982767676224189\n",
      "Bin Threshold 0.5982767676224189 0.4181131756740041 0.23794958372558933 0.0001\n",
      "---->>>  838 838\n"
     ]
    }
   ],
   "source": [
    "from LRsentiA_Sp_caPAZ import LexicalAnalyzer\n",
    "r = LexicalAnalyzer(DataName)\n",
    "predictions, pred_confidence_scores = r.classify_binary_dataset(X)\n",
    "df1, df2, df3, df4, df5 = r.distribute_predictions_into_bins(X,predictions, pred_confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "autonomaBucaramanga_DigitalMedio\n",
      "0.7911694510739857\n"
     ]
    }
   ],
   "source": [
    "from SSentiA_caPAZ import sSentiA_Sp\n",
    "s = sSentiA_Sp(DataName)\n",
    "s.apply_SSSentiA(df1, df2, df3, df4, df5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
